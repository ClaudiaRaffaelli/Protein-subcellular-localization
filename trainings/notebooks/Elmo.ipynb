{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Elmo.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "aB-kkfjYBsqE",
    "SD43ZOpEeBJe",
    "rMdrztYyCxlv",
    "A2ycNnntBoTr",
    "4hKIaacMjQFs",
    "xWso-MVkIe24",
    "J3Z9BJt6uu2F"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "WADhwgGPXMiR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "39a8893e-4db8-45a2-f4e3-6f5afb1a921f"
   },
   "source": [
    "%tensorflow_version 1.x"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "btAVRbiBB7nC"
   },
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, matthews_corrcoef\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import time\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import scipy.spatial.distance as ds\n",
    "import json\n",
    "\n",
    "def progress(value, max=100):\n",
    "    return HTML(\"\"\"\n",
    "        <progress\n",
    "            value='{value}'\n",
    "            max='{max}',\n",
    "            style='width: 100%'\n",
    "        >\n",
    "            {value}\n",
    "        </progress>\n",
    "    \"\"\".format(value=value, max=max))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1u2BMJnlGaXg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "54bde8f9-dec2-4513-ed9f-cb6bfc6abca2"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive', force_remount=True)\n",
    "path = \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/\""
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F3a2JpSJ86Ai"
   },
   "source": [
    "# Hyperparameters\n",
    "n = 3\n",
    "stride = 3"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB-kkfjYBsqE"
   },
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhL-4L5-CAUz"
   },
   "source": [
    "Extract sequences"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hgi3U9Cz2IrC"
   },
   "source": [
    "dataset_swiss = path + \"data/swissProt.fasta\"\n",
    "dataset_deeploc = path + \"data/deeploc_data.fasta\"\n",
    "\n",
    "with open(dataset_swiss, \"r\") as f:\n",
    "  lines = f.readlines()\n",
    "  sequences_swiss = [seq.replace(\"\\n\",\"\") for i,seq in enumerate(lines) if i%2!=0]\n",
    "with open(dataset_deeploc, \"r\") as f:\n",
    "  lines = f.readlines()\n",
    "  sequences_deeploc = [seq.replace(\"\\n\",\"\") for i,seq in enumerate(lines) if i%2!=0]\n",
    "sequences = sequences_swiss + sequences_deeploc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsLb8QCLB-iO"
   },
   "source": [
    "Extract tokens from sequences"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nOL3mjmM5cwt",
    "outputId": "e99134f5-f32e-4cbc-b93d-2fb10790377e"
   },
   "source": [
    "slice_size = 1000\n",
    "progressbar = display(progress(0, len(sequences)), display_id=True) # progress bar\n",
    "token_counter = Counter()\n",
    "\n",
    "for batch in range(0,len(sequences), slice_size):\n",
    "  tokens = []\n",
    "  for seq in sequences[batch:batch+slice_size]:\n",
    "    x = [seq[i:i+n] for i in range(0,len(seq), stride)]\n",
    "    tokens.append(x)\n",
    "  tokens = np.concatenate(tokens, axis = 0)\n",
    "  tokens = [x for x in tokens if len(x)==n]\n",
    "  token_counter += Counter(tokens)\n",
    "  progressbar.update(progress(batch, len(sequences)))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='578000'\n",
       "            max='578642',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            578000\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RI-LNWvEQDk"
   },
   "source": [
    "Save vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Oo8nWPeeCLfN"
   },
   "source": [
    "with open(path + \"data/vocabulary_\"+str(n)+\"_\"+str(stride)+\".txt\", \"w+\") as f:\n",
    "  tokens_name = np.array(list(token_counter.most_common()))[:,0]\n",
    "  tokens_name = np.insert(tokens_name, 0, ['<S>', '</S>', '<UNK>'])\n",
    "  for token in tokens_name:\n",
    "    f.write(token+\"\\n\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD43ZOpEeBJe"
   },
   "source": [
    "# Create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lTUZXUf-eDvk"
   },
   "source": [
    "dataset = path + \"data/swissProt.fasta\"\n",
    "\n",
    "with open(dataset, \"r\") as f:\n",
    "  lines = f.readlines()\n",
    "  sequences = [seq.replace(\"\\n\",\"\") for i,seq in enumerate(lines) if i%2!=0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ghQw8iPaeWZT",
    "outputId": "7e610d5b-dbb2-493d-af23-7a29b183119a"
   },
   "source": [
    "slice_size = 100\n",
    "progressbar = display(progress(0, len(sequences)), display_id=True) # progress bar\n",
    "\n",
    "\n",
    "for i, batch in enumerate(range(0,int(len(sequences)), slice_size)):\n",
    "  tokens = []\n",
    "  if not os.path.exists(path + \"data/training_\"+str(n)+\"_\"+str(stride)+\"/\"):\n",
    "    os.makedirs(path + \"data/training_\"+str(n)+\"_\"+str(stride)+\"/\")\n",
    "  with open(path + \"data/training_\"+str(n)+\"_\"+str(stride)+\"/\"+str(i)+\".txt\", \"w+\") as f:\n",
    "    for i, seq in enumerate(sequences[batch:batch+slice_size]):\n",
    "      x = [seq[i:i+n] for i in range(0,len(seq), stride)]\n",
    "      for n_gram in x:\n",
    "        if len(n_gram) == n:\n",
    "          f.write(n_gram+\" \")\n",
    "      if i != slice_size-1:\n",
    "        f.write(\"\\n\")\n",
    "  progressbar.update(progress(batch, len(sequences)+1))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "        <progress\n",
       "            value='564600'\n",
       "            max='564639',\n",
       "            style='width: 100%'\n",
       "        >\n",
       "            564600\n",
       "        </progress>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMdrztYyCxlv"
   },
   "source": [
    "# Elmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2ycNnntBoTr"
   },
   "source": [
    "\n",
    "\n",
    "> ## Install\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rLosgmzJnyjR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e830666-f101-4451-cc74-074f1bfd3a9f"
   },
   "source": [
    "!git clone https://github.com/allenai/bilm-tf.git && mv bilm-tf/ bilmtf"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Cloning into 'bilm-tf'...\n",
      "remote: Enumerating objects: 292, done.\u001B[K\n",
      "remote: Total 292 (delta 0), reused 0 (delta 0), pack-reused 292\u001B[K\n",
      "Receiving objects: 100% (292/292), 588.40 KiB | 9.05 MiB/s, done.\n",
      "Resolving deltas: 100% (137/137), done.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XLJJrDWqjehp"
   },
   "source": [
    "!cd bilmtf/ && python setup.py install"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hKIaacMjQFs"
   },
   "source": [
    "\n",
    "\n",
    "> ## Train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FWx3WWoFcLJ"
   },
   "source": [
    "Get n_train_tokens:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZnzZk2JDQgx",
    "outputId": "c55644de-b696-4621-b151-53986502e933"
   },
   "source": [
    "n_train_tokens_ = 0\n",
    "training_path = \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/training_\"+str(n)+\"_\"+str(stride)+\"/\"\n",
    "training_files = [f for f in listdir(training_path) if isfile(join(training_path, f))]\n",
    "\n",
    "for file_name in training_files:\n",
    "  with open(path + \"data/training_\"+str(n)+\"_\"+str(stride)+\"/\"+file_name, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line_i in lines:\n",
    "      n_train_tokens_ += len(line_i.split(\" \")) - 1\n",
    "n_train_tokens_"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "67650480"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Osil_mV3DWxi"
   },
   "source": [
    "Get n_vocab_tokens (probably unnecessary):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5lSrKTNDWD2",
    "outputId": "7d04445b-4a18-40ba-e491-fd93e871fba6"
   },
   "source": [
    "n_vocab_tokens = 0\n",
    "vocab_path = \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/vocabulary_\"+str(n)+\"_\"+str(stride)+\".txt\"\n",
    "\n",
    "with open(vocab_path, \"r\") as f:\n",
    "  lines = f.readlines()\n",
    "  n_vocab_tokens = len(lines)\n",
    "n_vocab_tokens"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9544"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWso-MVkIe24"
   },
   "source": [
    "> ### train_elmo.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS5YFMpYFmtZ"
   },
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xf4Z3fz6IczA"
   },
   "source": [
    "import argparse\n",
    "\n",
    "from bilmtf.bilm.training import train, load_options_latest_checkpoint, load_vocab\n",
    "from bilmtf.bilm.data import BidirectionalLMDataset\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # load the vocab\n",
    "    vocab = load_vocab(args[1], 50)\n",
    "\n",
    "    # define the options\n",
    "    batch_size = 128  # batch size for each GPU\n",
    "    n_gpus = 1\n",
    "\n",
    "    # number of tokens in training data (this for 1B Word Benchmark)\n",
    "    n_train_tokens = n_train_tokens_\n",
    "\n",
    "    options = {\n",
    "     'bidirectional': True,\n",
    "\n",
    "     'char_cnn': {'activation': 'relu',\n",
    "      'embedding': {'dim': 16},\n",
    "      'filters': [[1, 32],\n",
    "       [2, 32],\n",
    "       [3, 64],\n",
    "       [4, 128],\n",
    "       [5, 256],\n",
    "       [6, 512],\n",
    "       [7, 1024]],\n",
    "      'max_characters_per_token': 50,\n",
    "      'n_characters': 261,\n",
    "      'n_highway': 2},\n",
    "    \n",
    "     'dropout': 0.1,\n",
    "    \n",
    "     'lstm': {\n",
    "      'cell_clip': 3,\n",
    "      'dim': 4096,\n",
    "      'n_layers': 2,\n",
    "      'proj_clip': 3,\n",
    "      'projection_dim': 512,\n",
    "      'use_skip_connections': True},\n",
    "    \n",
    "     'all_clip_norm_val': 10.0,\n",
    "    \n",
    "     'n_epochs': 1,\n",
    "     'n_train_tokens': n_train_tokens,\n",
    "     'batch_size': batch_size,\n",
    "     'n_tokens_vocab': vocab.size,\n",
    "     'unroll_steps': 20,\n",
    "     'n_negative_samples_batch': 1,\n",
    "    }\n",
    "\n",
    "    prefix = args[2]\n",
    "    data = BidirectionalLMDataset(prefix, vocab, test=False,\n",
    "                                      shuffle_on_load=True)\n",
    "\n",
    "    tf_save_dir = args[0]\n",
    "    tf_log_dir = args[0]\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "      train(options, data, n_gpus, tf_save_dir, tf_log_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = [\"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/checkpoint_\"+str(n)+\"_\"+str(stride)+\"/\",\n",
    "            \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/vocabulary_\"+str(n)+\"_\"+str(stride)+\".txt\",\n",
    "            \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/training_\"+str(n)+\"_\"+str(stride)+\"/*\"]\n",
    "    \n",
    "    # create checkpoint folder if it doesn't exist\n",
    "    if not os.path.exists(path + \"data/checkpoint_\"+str(n)+\"_\"+str(stride)+\"/\"):\n",
    "      os.makedirs(path + \"data/checkpoint_\"+str(n)+\"_\"+str(stride)+\"/\")\n",
    "\n",
    "    main(args)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkowQu_IQTXx"
   },
   "source": [
    "Save weights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LQ7SbiQa9AV2"
   },
   "source": [
    "# Command for lab pc\n",
    "# cd bilmtf/ && python bin/dump_weights.py --save_dir \"../data/checkpoint_3_1/\" --outfile \"../data/weights/weights_3_1.hdf5\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H-CImkEiJlm3"
   },
   "source": [
    "!cd bilmtf/ && python bin/dump_weights.py --save_dir \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/checkpoint_{n}_{stride}/\" --outfile \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/weights/weights_{n}_{stride}.hdf5\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5_UsLk4J02g"
   },
   "source": [
    "> ### Evaluate (not working in colab)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "j0IIPkw7J0QD"
   },
   "source": [
    "!cd bilmtf/ && python bin/run_test.py \\\n",
    "    --test_prefix=\"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/training_{n}_{stride}/*\" \\\n",
    "    --vocab_file \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/vocabulary_{n}_{stride}.txt\" \\\n",
    "    --save_dir \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/checkpoint_{n}_{stride}/\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3VDgvmC2FJ0k"
   },
   "source": [
    "# command for lab pc\n",
    "# cd bilmtf/ && python bin/run_test.py --test_prefix=\"../data/heldout_3_1/*\" --vocab_file \"../data/vocabulary_3_1.txt\" --save_dir \"../data/checkpoint_3_1/\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3Z9BJt6uu2F"
   },
   "source": [
    "\n",
    "\n",
    "> ## Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtNwhh96ivWN"
   },
   "source": [
    "n_characters must be changed from 261 to 262 before prediction"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bwQU-_O0hQw-"
   },
   "source": [
    "options_path = \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/checkpoint_\"+str(n)+\"_\"+str(stride)+\"/options.json\"\n",
    "\n",
    "with open(options_path) as f:\n",
    "  options = json.loads(f.read())\n",
    "options['char_cnn']['n_characters'] = 262\n",
    "\n",
    "with open(options_path, 'w') as json_file:\n",
    "  json.dump(options, json_file)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snP9iA_KShzM"
   },
   "source": [
    "Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h-E6akngCELE"
   },
   "source": [
    "from bilmtf.bilm import Batcher, BidirectionalLanguageModel, weight_layers\n",
    "\n",
    "\n",
    "# Location of pretrained LM.  Here we use the test fixtures.\n",
    "vocab_file = \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/vocabulary_\"+str(n)+\"_\"+str(stride)+\".txt\"\n",
    "options_file = \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/checkpoint_\"+str(n)+\"_\"+str(stride)+\"/options.json\"\n",
    "weight_file = \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/weights/weights_\"+str(n)+\"_\"+str(stride)+\".hdf5\"\n",
    " \n",
    "# Create a Batcher to map text to character ids.\n",
    "batcher = Batcher(vocab_file, 50)\n",
    " \n",
    "# Input placeholders to the biLM.\n",
    "context_character_ids = tf.placeholder('int32', shape=(None, None, 50))\n",
    " \n",
    "# Build the biLM graph.\n",
    "bilm = BidirectionalLanguageModel(options_file, weight_file)\n",
    " \n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "  # Get ops to compute the LM embeddings.\n",
    "  context_embeddings_op = bilm(context_character_ids)\n",
    " \n",
    "  # Get an op to compute ELMo (weighted average of the internal biLM layers)\n",
    "  elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MMCNpGjSnoi"
   },
   "source": [
    "Get proteins embedding from deeploc_data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jVDVqPgBSdUK"
   },
   "source": [
    "deeploc_file = \"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/deeploc_data.fasta\"\n",
    "\n",
    "labels_dic_location = {\n",
    "    'Cell.membrane': 0,\n",
    "    'Cytoplasm': 1,\n",
    "    'Endoplasmic.reticulum': 2,\n",
    "    'Golgi.apparatus': 3,\n",
    "    'Lysosome/Vacuole': 4,\n",
    "    'Mitochondrion': 5,\n",
    "    'Nucleus': 6,\n",
    "    'Peroxisome': 7,\n",
    "    'Plastid': 8,\n",
    "    'Extracellular': 9\n",
    "}\n",
    "\n",
    "\n",
    "# Now we can compute embeddings.\n",
    "with open(deeploc_file, \"r\") as f:\n",
    "  lines = f.readlines()\n",
    "  sequences_deeploc = [seq.replace(\"\\n\",\"\") for i,seq in enumerate(lines) if i%2!=0]\n",
    "  headers_deeploc = [seq.replace(\"\\n\",\"\") for i,seq in enumerate(lines) if i%2==0]\n",
    "\n",
    "tokenized_context = []\n",
    "for seq in sequences_deeploc:\n",
    "  x = [seq[i:i+n] for i in range(0,len(seq), stride)]\n",
    "  tokenized_context.append(x)\n",
    "print(len(tokenized_context))\n",
    "\n",
    "embeddings_path = \"./data/embeddings_\"+str(n)+\"_\"+str(stride)+\"/\"\n",
    "slice_size = 1 # Don't change this value\n",
    "\n",
    "# restart from where you left\n",
    "# create embeddings folder if it doesn't exist\n",
    "if not os.path.exists(embeddings_path):\n",
    "  os.makedirs(embeddings_path)\n",
    "if os.path.isfile(embeddings_path+\"sequence_completed_\"+str(n)+\"_\"+str(stride)+\".txt\"):\n",
    "  with open(embeddings_path+\"sequence_completed_\"+str(n)+\"_\"+str(stride)+\".txt\", \"r\") as f:\n",
    "    sequence_completed = int(f.readline())\n",
    "  X_train = np.load(embeddings_path+\"X_train_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "  y_train_subcellular = np.load(embeddings_path+\"y_train_subcellular_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "  y_train_membrane = np.load(embeddings_path+\"y_train_membrane_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "  X_test = np.load(embeddings_path+\"X_test_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "  y_test_subcellular = np.load(embeddings_path+\"y_test_subcellular_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "  y_test_membrane = np.load(embeddings_path+\"y_test_membrane_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "else:\n",
    "  sequence_completed = 0\n",
    "  X_train = np.zeros((1,1024))\n",
    "  y_train_subcellular = np.ones((1))*99\n",
    "  y_train_membrane = np.ones((1))\n",
    "  X_test = np.zeros((1,1024))\n",
    "  y_test_subcellular = np.ones((1))*99\n",
    "  y_test_membrane = np.ones((1))*99\n",
    "\n",
    "#start extracting\n",
    "with tf.compat.v1.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "  # It is necessary to initialize variables once before running inference.\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i, batch in enumerate(range(sequence_completed,len(tokenized_context), slice_size)):\n",
    "    tokens = tokenized_context[batch:batch+slice_size]\n",
    "    header_tokens = headers_deeploc[batch:batch+slice_size][0]\n",
    "    print(header_tokens)\n",
    "    headers_class = [val for key,val in labels_dic_location.items() if key in header_tokens]\n",
    "    if \"-U\" not in header_tokens and len(headers_class) == 1:\n",
    "      # Create batches of data.\n",
    "      context_ids = batcher.batch_sentences(tokens)\n",
    "      #print(\"Shape of context ids = \", context_ids.shape)\n",
    "\n",
    "      # Compute ELMo representations (here for the input only, for simplicity).\n",
    "      elmo_context_input_ = sess.run(\n",
    "          elmo_context_input['weighted_op'],\n",
    "          feed_dict={context_character_ids: context_ids}\n",
    "      )\n",
    "      elmo_context_input_ = sess.run(tf.reduce_mean(elmo_context_input_, 1))\n",
    "      print(\"Shape of generated embeddings = \",elmo_context_input_.shape)\n",
    "\n",
    "      # Save\n",
    "      if \"test\" not in header_tokens:\n",
    "        X_train = np.concatenate((X_train, elmo_context_input_), axis=0)\n",
    "        y_train_subcellular = np.concatenate((y_train_subcellular, headers_class), axis=0)\n",
    "        np.save(embeddings_path+\"X_train_\"+str(n)+\"_\"+str(stride)+\".npy\", X_train)\n",
    "        np.save(embeddings_path+\"y_train_subcellular_\"+str(n)+\"_\"+str(stride)+\".npy\", y_train_subcellular)\n",
    "        if \"-M\" in header_tokens:\n",
    "          y_train_membrane = np.concatenate((y_train_membrane, [0]), axis=0)\n",
    "        else:\n",
    "          y_train_membrane = np.concatenate((y_train_membrane, [1]), axis=0)\n",
    "        np.save(embeddings_path+\"y_train_membrane_\"+str(n)+\"_\"+str(stride)+\".npy\", y_train_membrane)\n",
    "      else:\n",
    "        X_test = np.concatenate((X_test, elmo_context_input_), axis=0)\n",
    "        y_test_subcellular = np.concatenate((y_test_subcellular, headers_class), axis=0)\n",
    "        np.save(embeddings_path+\"X_test_\"+str(n)+\"_\"+str(stride)+\".npy\", X_test)\n",
    "        np.save(embeddings_path+\"y_test_subcellular_\"+str(n)+\"_\"+str(stride)+\".npy\", y_test_subcellular)\n",
    "        if \"-M\" in header_tokens:\n",
    "          y_test_membrane = np.concatenate((y_test_membrane, [0]), axis=0)\n",
    "        else:\n",
    "          y_test_membrane = np.concatenate((y_test_membrane, [1]), axis=0)\n",
    "        np.save(embeddings_path+\"y_test_membrane_\"+str(n)+\"_\"+str(stride)+\".npy\", y_test_membrane)\n",
    "\n",
    "    sequence_completed += slice_size\n",
    "    with open(embeddings_path+\"sequence_completed_\"+str(n)+\"_\"+str(stride)+\".txt\", \"w+\") as f:\n",
    "      f.write(str(sequence_completed))\n",
    "\n",
    "    print(f\"Sequence completed {sequence_completed}/{len(tokenized_context)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5g3Lbp6vzOrw"
   },
   "source": [
    "embeddings_path = \"./data/embeddings_\"+str(n)+\"_\"+str(stride)+\"/\"\n",
    "\n",
    "X_train = np.load(embeddings_path+\"X_train_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "y_train_subcellular = np.load(embeddings_path+\"y_train_subcellular_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "y_train_membrane = np.load(embeddings_path+\"y_train_membrane_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "X_test = np.load(embeddings_path+\"X_test_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "y_test_subcellular = np.load(embeddings_path+\"y_test_subcellular_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "y_test_membrane = np.load(embeddings_path+\"y_test_membrane_\"+str(n)+\"_\"+str(stride)+\".npy\")\n",
    "\n",
    "np.savez_compressed(embeddings_path+\".train_\"+str(n)+\"_\"+str(stride), X_train=X_train, y_train_location=y_train_subcellular, y_train_membrane=y_train_membrane)\n",
    "np.savez_compressed(embeddings_path+\".test_\"+str(n)+\"_\"+str(stride), X_test=X_test, y_test_location=y_test_subcellular, y_test_membrane=y_test_membrane)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URyI6ePMh3Ky"
   },
   "source": [
    "\n",
    "\n",
    "# Protein subcellular localization and membrane vs soluble classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X4R92bEIkNEe"
   },
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "  \"\"\" Implementing a layer that does attention according to Bahdanau style \"\"\"\n",
    "\n",
    "  def __init__(self, units):\n",
    "      super(Attention, self).__init__()\n",
    "      # W1 weight of the previously hidden state(hidden_size x hidden_size)\n",
    "      self.W1 = tf.keras.layers.Dense(units)\n",
    "      # W2 weight for all the encoder hidden states\n",
    "      self.W2 = tf.keras.layers.Dense(units)\n",
    "      self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs, hidden):\n",
    "      # 'hidden' (h_t) is expanded over the time axis to prepare it for the addition\n",
    "      # that follows. hidden will always be the last hidden state of the RNN.\n",
    "      # (in seq2seq in would have been the current state of the decoder step)\n",
    "      # 'features' (h_s) are all the hidden states of the encoder.\n",
    "      hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "      # Bahdanau additive style to compute the score:\n",
    "      # score = v_a * tanh(W_1*h_t + W_2*h_s)\n",
    "      score = tf.nn.tanh(self.W1(inputs) + self.W2(hidden_with_time_axis))\n",
    "      attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "      context_vector = attention_weights * inputs\n",
    "      context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "      return context_vector, attention_weights"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "om4G2EspSFQJ"
   },
   "source": [
    "import math\n",
    "\n",
    "def create_CNN_LSTM_Attention_complete():\n",
    "  n_hid=20\n",
    "  lr=0.0005\n",
    "  drop_prob=0.6\n",
    "  drop_hid=0.1\n",
    "  n_filt=40\n",
    "  n_feat=1024\n",
    "  n_membrane_class=2\n",
    "\n",
    "\n",
    "  inputs = keras.Input(shape=(1, n_feat))\n",
    "\n",
    "  l_drop1 = layers.Dropout(drop_prob)(inputs)\n",
    "\n",
    "  # Size of convolutional layers\n",
    "  f_size_a = 1\n",
    "  f_size_b = 3\n",
    "  f_size_c = 5\n",
    "  f_size_d = 9\n",
    "  f_size_e = 15\n",
    "  f_size_f = 21\n",
    "\n",
    "  # initialization with random orthogonal weights using sqrt(2) for rectified linear units as scaling factor\n",
    "  initializer = tf.keras.initializers.Orthogonal(gain=math.sqrt(2))\n",
    "\n",
    "  l_conv_a = layers.Conv1D(n_filt, f_size_a, strides=1, padding=\"same\", kernel_initializer=initializer,\n",
    "                            activation=\"relu\", data_format='channels_first')(l_drop1)\n",
    "  l_conv_b = layers.Conv1D(n_filt, f_size_b, strides=1, padding=\"same\", kernel_initializer=initializer,\n",
    "                            activation=\"relu\", data_format='channels_first')(l_drop1)\n",
    "  l_conv_c = layers.Conv1D(n_filt, f_size_c, strides=1, padding=\"same\", kernel_initializer=initializer,\n",
    "                            activation=\"relu\", data_format='channels_first')(l_drop1)\n",
    "  l_conv_d = layers.Conv1D(n_filt, f_size_d, strides=1, padding=\"same\", kernel_initializer=initializer,\n",
    "                            activation=\"relu\", data_format='channels_first')(l_drop1)\n",
    "  l_conv_e = layers.Conv1D(n_filt, f_size_e, strides=1, padding=\"same\", kernel_initializer=initializer,\n",
    "                            activation=\"relu\", data_format='channels_first')(l_drop1)\n",
    "  l_conv_f = layers.Conv1D(n_filt, f_size_f, strides=1, padding=\"same\", kernel_initializer=initializer,\n",
    "                            activation=\"relu\", data_format='channels_first')(l_drop1)\n",
    "\n",
    "  # concatenate all convolutional layers\n",
    "  l_conc = tf.keras.layers.Concatenate(axis=1)([l_conv_a, l_conv_b, l_conv_c, l_conv_d, l_conv_e, l_conv_f])\n",
    "\n",
    "  l_conv_final = layers.Conv1D(\n",
    "      filters=128, kernel_size=f_size_b, strides=1, padding=\"same\", activation=\"relu\",\n",
    "      data_format='channels_first')(l_conc)\n",
    "\n",
    "  # encoders LSTM\n",
    "  l_lstm, forward_h, forward_c, backward_h, backward_c = layers.Bidirectional \\\n",
    "      (layers.LSTM(n_hid, dropout=drop_hid, return_sequences=True, return_state=True,\n",
    "                    activation=\"tanh\")) \\\n",
    "      (l_conv_final)\n",
    "  state_h = layers.Concatenate()([forward_h, backward_h])\n",
    "  state_c = layers.Concatenate()([forward_c, backward_c])\n",
    "\n",
    "  # Set up the attention layer\n",
    "  context_vector, attention_weights = Attention(n_hid * 2)(l_lstm, state_h)\n",
    "\n",
    "  l_drop2 = layers.Dropout(drop_hid)(context_vector)\n",
    "\n",
    "  l_dense = layers.Dense(n_hid * 2, activation=\"relu\", kernel_initializer=initializer)(l_drop2)\n",
    "\n",
    "  l_drop3 = layers.Dropout(drop_hid)(l_dense)\n",
    "\n",
    "  l_out_subcellular = layers.Dense(n_class, activation=\"softmax\", name=\"subcellular\")(l_drop3)\n",
    "  l_out_membrane = layers.Dense(n_membrane_class, activation=\"softmax\", name=\"membrane\")(l_drop3)\n",
    "  model = keras.Model(inputs, [l_out_subcellular, l_out_membrane])\n",
    "\n",
    "  # gradient clipping clips parameters' gradients during backprop by a maximum value of 2\n",
    "  # with clipnorm the gradients will be clipped when their L2 norm exceeds this value.\n",
    "  model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
    "                      optimizer=optimizers.Adam(learning_rate=lr, clipvalue=2, clipnorm=3),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "  # setting initial state tensors to be passed to the first call of the cell (cell init and hid init in\n",
    "  # bidirectional LSTM)\n",
    "  model.layers[12].initial_states = [tf.keras.initializers.Orthogonal(), tf.keras.initializers.Orthogonal()]\n",
    "\n",
    "  return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UZwIXmJ4h8_8"
   },
   "source": [
    "train = np.load(\"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/noSecVeq/embeddings_\"+str(n)+\"_\"+str(stride)+\"/train_\"+str(n)+\"_\"+str(stride)+\".npz\")\n",
    "X_train = train['X_train'][1:]\n",
    "y_train_subcellular = train['y_train_location'][1:]\n",
    "y_train_membrane = train['y_train_membrane'][1:]\n",
    "\n",
    "validation = np.load(\"/gdrive/My Drive/ProteinsML/Protein-subcellular-localization/Elmo/data/noSecVeq/embeddings_\"+str(n)+\"_\"+str(stride)+\"/test_\"+str(n)+\"_\"+str(stride)+\".npz\")\n",
    "X_val = validation['X_test'][1:]\n",
    "y_val_subcellular = validation['y_test_location'][1:]\n",
    "y_val_membrane = validation['y_test_membrane'][1:]\n",
    "\n",
    "\n",
    "# One-hot encoding\n",
    "n_class = 10\n",
    "y_train_subcellular = to_categorical(y_train_subcellular, n_class)\n",
    "y_train_membrane = to_categorical(y_train_membrane, 2)\n",
    "y_val_subcellular = to_categorical(y_val_subcellular, n_class)\n",
    "y_val_membrane = to_categorical(y_val_membrane, 2)\n",
    "\n",
    "X_train = np.reshape(X_train, (6913, 1, 1024))\n",
    "X_val = np.reshape(X_val, (1749, 1, 1024))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfDmxBg_jFG6",
    "outputId": "17049679-00c6-41b9-bd32-15d8c9d7509a"
   },
   "source": [
    "# creating the model\n",
    "model = create_CNN_LSTM_Attention_complete()\n",
    "\n",
    "n_epochs = 120\n",
    "\n",
    "history = model.fit(X_train, [y_train_subcellular, y_train_membrane], validation_data=(X_val, [y_val_subcellular, y_val_membrane]), epochs=n_epochs, batch_size=128)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "55/55 [==============================] - 41s 88ms/step - loss: 2.6796 - subcellular_loss: 1.9991 - membrane_loss: 0.6805 - subcellular_accuracy: 0.2811 - membrane_accuracy: 0.5801 - val_loss: 2.6200 - val_subcellular_loss: 1.9467 - val_membrane_loss: 0.6734 - val_subcellular_accuracy: 0.3225 - val_membrane_accuracy: 0.5752\n",
      "Epoch 2/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.5525 - subcellular_loss: 1.9039 - membrane_loss: 0.6485 - subcellular_accuracy: 0.3298 - membrane_accuracy: 0.6220 - val_loss: 2.4536 - val_subcellular_loss: 1.8316 - val_membrane_loss: 0.6220 - val_subcellular_accuracy: 0.3528 - val_membrane_accuracy: 0.6615\n",
      "Epoch 3/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.4494 - subcellular_loss: 1.8202 - membrane_loss: 0.6293 - subcellular_accuracy: 0.3906 - membrane_accuracy: 0.6522 - val_loss: 2.4468 - val_subcellular_loss: 1.8235 - val_membrane_loss: 0.6233 - val_subcellular_accuracy: 0.4288 - val_membrane_accuracy: 0.6529\n",
      "Epoch 4/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.3283 - subcellular_loss: 1.7392 - membrane_loss: 0.5891 - subcellular_accuracy: 0.4235 - membrane_accuracy: 0.6915 - val_loss: 2.4309 - val_subcellular_loss: 1.8057 - val_membrane_loss: 0.6251 - val_subcellular_accuracy: 0.4540 - val_membrane_accuracy: 0.6438\n",
      "Epoch 5/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.2413 - subcellular_loss: 1.6714 - membrane_loss: 0.5698 - subcellular_accuracy: 0.4547 - membrane_accuracy: 0.7094 - val_loss: 2.3713 - val_subcellular_loss: 1.7616 - val_membrane_loss: 0.6097 - val_subcellular_accuracy: 0.4551 - val_membrane_accuracy: 0.6667\n",
      "Epoch 6/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.2057 - subcellular_loss: 1.6466 - membrane_loss: 0.5591 - subcellular_accuracy: 0.4613 - membrane_accuracy: 0.7117 - val_loss: 2.3671 - val_subcellular_loss: 1.7619 - val_membrane_loss: 0.6052 - val_subcellular_accuracy: 0.4860 - val_membrane_accuracy: 0.6661\n",
      "Epoch 7/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.1451 - subcellular_loss: 1.6001 - membrane_loss: 0.5450 - subcellular_accuracy: 0.4788 - membrane_accuracy: 0.7226 - val_loss: 2.2591 - val_subcellular_loss: 1.6919 - val_membrane_loss: 0.5672 - val_subcellular_accuracy: 0.4768 - val_membrane_accuracy: 0.7158\n",
      "Epoch 8/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.1153 - subcellular_loss: 1.5825 - membrane_loss: 0.5328 - subcellular_accuracy: 0.4818 - membrane_accuracy: 0.7357 - val_loss: 2.2731 - val_subcellular_loss: 1.6910 - val_membrane_loss: 0.5822 - val_subcellular_accuracy: 0.4934 - val_membrane_accuracy: 0.6827\n",
      "Epoch 9/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.0978 - subcellular_loss: 1.5708 - membrane_loss: 0.5270 - subcellular_accuracy: 0.4859 - membrane_accuracy: 0.7324 - val_loss: 2.1941 - val_subcellular_loss: 1.6443 - val_membrane_loss: 0.5498 - val_subcellular_accuracy: 0.4911 - val_membrane_accuracy: 0.7347\n",
      "Epoch 10/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 2.0711 - subcellular_loss: 1.5537 - membrane_loss: 0.5174 - subcellular_accuracy: 0.4912 - membrane_accuracy: 0.7403 - val_loss: 2.2029 - val_subcellular_loss: 1.6521 - val_membrane_loss: 0.5507 - val_subcellular_accuracy: 0.4843 - val_membrane_accuracy: 0.7530\n",
      "Epoch 11/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 2.0500 - subcellular_loss: 1.5345 - membrane_loss: 0.5155 - subcellular_accuracy: 0.5008 - membrane_accuracy: 0.7396 - val_loss: 2.2733 - val_subcellular_loss: 1.7035 - val_membrane_loss: 0.5698 - val_subcellular_accuracy: 0.4700 - val_membrane_accuracy: 0.7073\n",
      "Epoch 12/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 2.0410 - subcellular_loss: 1.5309 - membrane_loss: 0.5101 - subcellular_accuracy: 0.5014 - membrane_accuracy: 0.7461 - val_loss: 2.2364 - val_subcellular_loss: 1.6664 - val_membrane_loss: 0.5700 - val_subcellular_accuracy: 0.5083 - val_membrane_accuracy: 0.6958\n",
      "Epoch 13/120\n",
      "55/55 [==============================] - 4s 69ms/step - loss: 2.0214 - subcellular_loss: 1.5187 - membrane_loss: 0.5027 - subcellular_accuracy: 0.5064 - membrane_accuracy: 0.7500 - val_loss: 2.2158 - val_subcellular_loss: 1.6579 - val_membrane_loss: 0.5579 - val_subcellular_accuracy: 0.4946 - val_membrane_accuracy: 0.7113\n",
      "Epoch 14/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 1.9887 - subcellular_loss: 1.4933 - membrane_loss: 0.4955 - subcellular_accuracy: 0.5069 - membrane_accuracy: 0.7493 - val_loss: 2.1628 - val_subcellular_loss: 1.6227 - val_membrane_loss: 0.5401 - val_subcellular_accuracy: 0.5037 - val_membrane_accuracy: 0.7581\n",
      "Epoch 15/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 1.9788 - subcellular_loss: 1.4876 - membrane_loss: 0.4912 - subcellular_accuracy: 0.5135 - membrane_accuracy: 0.7581 - val_loss: 2.1679 - val_subcellular_loss: 1.6303 - val_membrane_loss: 0.5376 - val_subcellular_accuracy: 0.5163 - val_membrane_accuracy: 0.7616\n",
      "Epoch 16/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 1.9810 - subcellular_loss: 1.4836 - membrane_loss: 0.4974 - subcellular_accuracy: 0.5137 - membrane_accuracy: 0.7557 - val_loss: 2.1766 - val_subcellular_loss: 1.6308 - val_membrane_loss: 0.5457 - val_subcellular_accuracy: 0.5066 - val_membrane_accuracy: 0.7490\n",
      "Epoch 17/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 1.9649 - subcellular_loss: 1.4749 - membrane_loss: 0.4900 - subcellular_accuracy: 0.5183 - membrane_accuracy: 0.7609 - val_loss: 2.2017 - val_subcellular_loss: 1.6512 - val_membrane_loss: 0.5505 - val_subcellular_accuracy: 0.5077 - val_membrane_accuracy: 0.7621\n",
      "Epoch 18/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 1.9556 - subcellular_loss: 1.4717 - membrane_loss: 0.4839 - subcellular_accuracy: 0.5193 - membrane_accuracy: 0.7629 - val_loss: 2.2008 - val_subcellular_loss: 1.6516 - val_membrane_loss: 0.5492 - val_subcellular_accuracy: 0.5009 - val_membrane_accuracy: 0.7444\n",
      "Epoch 19/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.9559 - subcellular_loss: 1.4723 - membrane_loss: 0.4837 - subcellular_accuracy: 0.5179 - membrane_accuracy: 0.7629 - val_loss: 2.1978 - val_subcellular_loss: 1.6431 - val_membrane_loss: 0.5547 - val_subcellular_accuracy: 0.5037 - val_membrane_accuracy: 0.7227\n",
      "Epoch 20/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.9307 - subcellular_loss: 1.4471 - membrane_loss: 0.4837 - subcellular_accuracy: 0.5254 - membrane_accuracy: 0.7597 - val_loss: 2.1644 - val_subcellular_loss: 1.6273 - val_membrane_loss: 0.5372 - val_subcellular_accuracy: 0.5111 - val_membrane_accuracy: 0.7496\n",
      "Epoch 21/120\n",
      "55/55 [==============================] - 4s 70ms/step - loss: 1.9215 - subcellular_loss: 1.4432 - membrane_loss: 0.4783 - subcellular_accuracy: 0.5280 - membrane_accuracy: 0.7657 - val_loss: 2.2295 - val_subcellular_loss: 1.6660 - val_membrane_loss: 0.5635 - val_subcellular_accuracy: 0.5152 - val_membrane_accuracy: 0.7330\n",
      "Epoch 22/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.9152 - subcellular_loss: 1.4368 - membrane_loss: 0.4784 - subcellular_accuracy: 0.5252 - membrane_accuracy: 0.7709 - val_loss: 2.0989 - val_subcellular_loss: 1.5751 - val_membrane_loss: 0.5238 - val_subcellular_accuracy: 0.5254 - val_membrane_accuracy: 0.7650\n",
      "Epoch 23/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.9166 - subcellular_loss: 1.4400 - membrane_loss: 0.4766 - subcellular_accuracy: 0.5258 - membrane_accuracy: 0.7658 - val_loss: 2.2366 - val_subcellular_loss: 1.6781 - val_membrane_loss: 0.5585 - val_subcellular_accuracy: 0.5140 - val_membrane_accuracy: 0.7541\n",
      "Epoch 24/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.9146 - subcellular_loss: 1.4383 - membrane_loss: 0.4763 - subcellular_accuracy: 0.5313 - membrane_accuracy: 0.7599 - val_loss: 2.2230 - val_subcellular_loss: 1.6692 - val_membrane_loss: 0.5538 - val_subcellular_accuracy: 0.5232 - val_membrane_accuracy: 0.7473\n",
      "Epoch 25/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.9027 - subcellular_loss: 1.4236 - membrane_loss: 0.4792 - subcellular_accuracy: 0.5304 - membrane_accuracy: 0.7639 - val_loss: 2.2034 - val_subcellular_loss: 1.6494 - val_membrane_loss: 0.5539 - val_subcellular_accuracy: 0.5300 - val_membrane_accuracy: 0.7113\n",
      "Epoch 26/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.8948 - subcellular_loss: 1.4238 - membrane_loss: 0.4710 - subcellular_accuracy: 0.5291 - membrane_accuracy: 0.7691 - val_loss: 2.1712 - val_subcellular_loss: 1.6309 - val_membrane_loss: 0.5403 - val_subcellular_accuracy: 0.5009 - val_membrane_accuracy: 0.7381\n",
      "Epoch 27/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.8769 - subcellular_loss: 1.4105 - membrane_loss: 0.4664 - subcellular_accuracy: 0.5346 - membrane_accuracy: 0.7699 - val_loss: 2.1246 - val_subcellular_loss: 1.5888 - val_membrane_loss: 0.5357 - val_subcellular_accuracy: 0.5334 - val_membrane_accuracy: 0.7153\n",
      "Epoch 28/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.8751 - subcellular_loss: 1.4105 - membrane_loss: 0.4646 - subcellular_accuracy: 0.5412 - membrane_accuracy: 0.7725 - val_loss: 2.1484 - val_subcellular_loss: 1.6106 - val_membrane_loss: 0.5378 - val_subcellular_accuracy: 0.5283 - val_membrane_accuracy: 0.7433\n",
      "Epoch 29/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.8723 - subcellular_loss: 1.4092 - membrane_loss: 0.4631 - subcellular_accuracy: 0.5341 - membrane_accuracy: 0.7729 - val_loss: 2.1632 - val_subcellular_loss: 1.6248 - val_membrane_loss: 0.5384 - val_subcellular_accuracy: 0.5180 - val_membrane_accuracy: 0.7318\n",
      "Epoch 30/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.8717 - subcellular_loss: 1.4015 - membrane_loss: 0.4702 - subcellular_accuracy: 0.5370 - membrane_accuracy: 0.7655 - val_loss: 2.1286 - val_subcellular_loss: 1.5968 - val_membrane_loss: 0.5318 - val_subcellular_accuracy: 0.5306 - val_membrane_accuracy: 0.7782\n",
      "Epoch 31/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8575 - subcellular_loss: 1.3922 - membrane_loss: 0.4652 - subcellular_accuracy: 0.5375 - membrane_accuracy: 0.7719 - val_loss: 2.0952 - val_subcellular_loss: 1.5693 - val_membrane_loss: 0.5259 - val_subcellular_accuracy: 0.5455 - val_membrane_accuracy: 0.7650\n",
      "Epoch 32/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.8502 - subcellular_loss: 1.3900 - membrane_loss: 0.4602 - subcellular_accuracy: 0.5459 - membrane_accuracy: 0.7780 - val_loss: 1.9853 - val_subcellular_loss: 1.4941 - val_membrane_loss: 0.4913 - val_subcellular_accuracy: 0.5495 - val_membrane_accuracy: 0.7764\n",
      "Epoch 33/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8430 - subcellular_loss: 1.3844 - membrane_loss: 0.4586 - subcellular_accuracy: 0.5419 - membrane_accuracy: 0.7778 - val_loss: 2.1136 - val_subcellular_loss: 1.5857 - val_membrane_loss: 0.5279 - val_subcellular_accuracy: 0.5352 - val_membrane_accuracy: 0.7301\n",
      "Epoch 34/120\n",
      "55/55 [==============================] - 4s 71ms/step - loss: 1.8348 - subcellular_loss: 1.3755 - membrane_loss: 0.4593 - subcellular_accuracy: 0.5448 - membrane_accuracy: 0.7732 - val_loss: 2.0747 - val_subcellular_loss: 1.5553 - val_membrane_loss: 0.5194 - val_subcellular_accuracy: 0.5260 - val_membrane_accuracy: 0.7633\n",
      "Epoch 35/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8248 - subcellular_loss: 1.3686 - membrane_loss: 0.4562 - subcellular_accuracy: 0.5519 - membrane_accuracy: 0.7725 - val_loss: 1.9966 - val_subcellular_loss: 1.5010 - val_membrane_loss: 0.4956 - val_subcellular_accuracy: 0.5483 - val_membrane_accuracy: 0.7753\n",
      "Epoch 36/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8304 - subcellular_loss: 1.3738 - membrane_loss: 0.4566 - subcellular_accuracy: 0.5526 - membrane_accuracy: 0.7739 - val_loss: 2.0518 - val_subcellular_loss: 1.5366 - val_membrane_loss: 0.5152 - val_subcellular_accuracy: 0.5449 - val_membrane_accuracy: 0.7559\n",
      "Epoch 37/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8223 - subcellular_loss: 1.3645 - membrane_loss: 0.4577 - subcellular_accuracy: 0.5516 - membrane_accuracy: 0.7739 - val_loss: 2.0427 - val_subcellular_loss: 1.5328 - val_membrane_loss: 0.5099 - val_subcellular_accuracy: 0.5374 - val_membrane_accuracy: 0.7770\n",
      "Epoch 38/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8207 - subcellular_loss: 1.3653 - membrane_loss: 0.4554 - subcellular_accuracy: 0.5497 - membrane_accuracy: 0.7787 - val_loss: 2.0767 - val_subcellular_loss: 1.5592 - val_membrane_loss: 0.5175 - val_subcellular_accuracy: 0.5312 - val_membrane_accuracy: 0.7484\n",
      "Epoch 39/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8155 - subcellular_loss: 1.3581 - membrane_loss: 0.4573 - subcellular_accuracy: 0.5546 - membrane_accuracy: 0.7743 - val_loss: 2.0542 - val_subcellular_loss: 1.5405 - val_membrane_loss: 0.5137 - val_subcellular_accuracy: 0.5363 - val_membrane_accuracy: 0.7507\n",
      "Epoch 40/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7991 - subcellular_loss: 1.3462 - membrane_loss: 0.4529 - subcellular_accuracy: 0.5546 - membrane_accuracy: 0.7819 - val_loss: 2.0883 - val_subcellular_loss: 1.5615 - val_membrane_loss: 0.5268 - val_subcellular_accuracy: 0.5232 - val_membrane_accuracy: 0.7278\n",
      "Epoch 41/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8097 - subcellular_loss: 1.3581 - membrane_loss: 0.4516 - subcellular_accuracy: 0.5526 - membrane_accuracy: 0.7771 - val_loss: 2.1862 - val_subcellular_loss: 1.6403 - val_membrane_loss: 0.5459 - val_subcellular_accuracy: 0.5203 - val_membrane_accuracy: 0.7261\n",
      "Epoch 42/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8239 - subcellular_loss: 1.3660 - membrane_loss: 0.4579 - subcellular_accuracy: 0.5478 - membrane_accuracy: 0.7755 - val_loss: 2.1330 - val_subcellular_loss: 1.5996 - val_membrane_loss: 0.5334 - val_subcellular_accuracy: 0.5352 - val_membrane_accuracy: 0.7507\n",
      "Epoch 43/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.8149 - subcellular_loss: 1.3601 - membrane_loss: 0.4548 - subcellular_accuracy: 0.5574 - membrane_accuracy: 0.7768 - val_loss: 2.2241 - val_subcellular_loss: 1.6654 - val_membrane_loss: 0.5587 - val_subcellular_accuracy: 0.5014 - val_membrane_accuracy: 0.6792\n",
      "Epoch 44/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7998 - subcellular_loss: 1.3455 - membrane_loss: 0.4543 - subcellular_accuracy: 0.5556 - membrane_accuracy: 0.7769 - val_loss: 2.1155 - val_subcellular_loss: 1.5898 - val_membrane_loss: 0.5257 - val_subcellular_accuracy: 0.5260 - val_membrane_accuracy: 0.7399\n",
      "Epoch 45/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7838 - subcellular_loss: 1.3358 - membrane_loss: 0.4481 - subcellular_accuracy: 0.5652 - membrane_accuracy: 0.7850 - val_loss: 2.0913 - val_subcellular_loss: 1.5730 - val_membrane_loss: 0.5183 - val_subcellular_accuracy: 0.5243 - val_membrane_accuracy: 0.7467\n",
      "Epoch 46/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7891 - subcellular_loss: 1.3407 - membrane_loss: 0.4484 - subcellular_accuracy: 0.5572 - membrane_accuracy: 0.7762 - val_loss: 2.1347 - val_subcellular_loss: 1.5873 - val_membrane_loss: 0.5474 - val_subcellular_accuracy: 0.5174 - val_membrane_accuracy: 0.6855\n",
      "Epoch 47/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7985 - subcellular_loss: 1.3486 - membrane_loss: 0.4499 - subcellular_accuracy: 0.5582 - membrane_accuracy: 0.7806 - val_loss: 2.1378 - val_subcellular_loss: 1.5986 - val_membrane_loss: 0.5392 - val_subcellular_accuracy: 0.5049 - val_membrane_accuracy: 0.7021\n",
      "Epoch 48/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7740 - subcellular_loss: 1.3252 - membrane_loss: 0.4488 - subcellular_accuracy: 0.5592 - membrane_accuracy: 0.7774 - val_loss: 2.0866 - val_subcellular_loss: 1.5616 - val_membrane_loss: 0.5250 - val_subcellular_accuracy: 0.5232 - val_membrane_accuracy: 0.7227\n",
      "Epoch 49/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7810 - subcellular_loss: 1.3323 - membrane_loss: 0.4487 - subcellular_accuracy: 0.5585 - membrane_accuracy: 0.7781 - val_loss: 2.1649 - val_subcellular_loss: 1.6227 - val_membrane_loss: 0.5422 - val_subcellular_accuracy: 0.5111 - val_membrane_accuracy: 0.7015\n",
      "Epoch 50/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7859 - subcellular_loss: 1.3345 - membrane_loss: 0.4514 - subcellular_accuracy: 0.5639 - membrane_accuracy: 0.7833 - val_loss: 2.1155 - val_subcellular_loss: 1.5803 - val_membrane_loss: 0.5352 - val_subcellular_accuracy: 0.5146 - val_membrane_accuracy: 0.7095\n",
      "Epoch 51/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7696 - subcellular_loss: 1.3220 - membrane_loss: 0.4476 - subcellular_accuracy: 0.5637 - membrane_accuracy: 0.7800 - val_loss: 2.1478 - val_subcellular_loss: 1.6001 - val_membrane_loss: 0.5477 - val_subcellular_accuracy: 0.4969 - val_membrane_accuracy: 0.6832\n",
      "Epoch 52/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7824 - subcellular_loss: 1.3349 - membrane_loss: 0.4475 - subcellular_accuracy: 0.5610 - membrane_accuracy: 0.7849 - val_loss: 2.0441 - val_subcellular_loss: 1.5347 - val_membrane_loss: 0.5094 - val_subcellular_accuracy: 0.5409 - val_membrane_accuracy: 0.7524\n",
      "Epoch 53/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7621 - subcellular_loss: 1.3191 - membrane_loss: 0.4429 - subcellular_accuracy: 0.5652 - membrane_accuracy: 0.7833 - val_loss: 2.0704 - val_subcellular_loss: 1.5519 - val_membrane_loss: 0.5185 - val_subcellular_accuracy: 0.5403 - val_membrane_accuracy: 0.7336\n",
      "Epoch 54/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7465 - subcellular_loss: 1.3054 - membrane_loss: 0.4411 - subcellular_accuracy: 0.5707 - membrane_accuracy: 0.7868 - val_loss: 2.1766 - val_subcellular_loss: 1.6142 - val_membrane_loss: 0.5625 - val_subcellular_accuracy: 0.4929 - val_membrane_accuracy: 0.6655\n",
      "Epoch 55/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7662 - subcellular_loss: 1.3202 - membrane_loss: 0.4460 - subcellular_accuracy: 0.5608 - membrane_accuracy: 0.7798 - val_loss: 2.0621 - val_subcellular_loss: 1.5470 - val_membrane_loss: 0.5151 - val_subcellular_accuracy: 0.5506 - val_membrane_accuracy: 0.7553\n",
      "Epoch 56/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7460 - subcellular_loss: 1.3034 - membrane_loss: 0.4427 - subcellular_accuracy: 0.5663 - membrane_accuracy: 0.7842 - val_loss: 2.0408 - val_subcellular_loss: 1.5308 - val_membrane_loss: 0.5100 - val_subcellular_accuracy: 0.5472 - val_membrane_accuracy: 0.7581\n",
      "Epoch 57/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7447 - subcellular_loss: 1.3009 - membrane_loss: 0.4437 - subcellular_accuracy: 0.5730 - membrane_accuracy: 0.7852 - val_loss: 2.0956 - val_subcellular_loss: 1.5607 - val_membrane_loss: 0.5349 - val_subcellular_accuracy: 0.5220 - val_membrane_accuracy: 0.6975\n",
      "Epoch 58/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7323 - subcellular_loss: 1.2982 - membrane_loss: 0.4341 - subcellular_accuracy: 0.5668 - membrane_accuracy: 0.7910 - val_loss: 2.0778 - val_subcellular_loss: 1.5540 - val_membrane_loss: 0.5237 - val_subcellular_accuracy: 0.5254 - val_membrane_accuracy: 0.7170\n",
      "Epoch 59/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7477 - subcellular_loss: 1.3104 - membrane_loss: 0.4373 - subcellular_accuracy: 0.5692 - membrane_accuracy: 0.7866 - val_loss: 2.2120 - val_subcellular_loss: 1.6445 - val_membrane_loss: 0.5675 - val_subcellular_accuracy: 0.4889 - val_membrane_accuracy: 0.6547\n",
      "Epoch 60/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7291 - subcellular_loss: 1.2916 - membrane_loss: 0.4375 - subcellular_accuracy: 0.5731 - membrane_accuracy: 0.7862 - val_loss: 2.1376 - val_subcellular_loss: 1.5949 - val_membrane_loss: 0.5427 - val_subcellular_accuracy: 0.5180 - val_membrane_accuracy: 0.6941\n",
      "Epoch 61/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7245 - subcellular_loss: 1.2902 - membrane_loss: 0.4343 - subcellular_accuracy: 0.5723 - membrane_accuracy: 0.7849 - val_loss: 2.0795 - val_subcellular_loss: 1.5634 - val_membrane_loss: 0.5162 - val_subcellular_accuracy: 0.5232 - val_membrane_accuracy: 0.7256\n",
      "Epoch 62/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7208 - subcellular_loss: 1.2868 - membrane_loss: 0.4340 - subcellular_accuracy: 0.5759 - membrane_accuracy: 0.7884 - val_loss: 2.0660 - val_subcellular_loss: 1.5444 - val_membrane_loss: 0.5216 - val_subcellular_accuracy: 0.5346 - val_membrane_accuracy: 0.7153\n",
      "Epoch 63/120\n",
      "55/55 [==============================] - 4s 72ms/step - loss: 1.7269 - subcellular_loss: 1.2911 - membrane_loss: 0.4358 - subcellular_accuracy: 0.5705 - membrane_accuracy: 0.7848 - val_loss: 2.0751 - val_subcellular_loss: 1.5554 - val_membrane_loss: 0.5197 - val_subcellular_accuracy: 0.5272 - val_membrane_accuracy: 0.7233\n",
      "Epoch 64/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7358 - subcellular_loss: 1.2981 - membrane_loss: 0.4377 - subcellular_accuracy: 0.5710 - membrane_accuracy: 0.7855 - val_loss: 2.1419 - val_subcellular_loss: 1.5921 - val_membrane_loss: 0.5498 - val_subcellular_accuracy: 0.5066 - val_membrane_accuracy: 0.6798\n",
      "Epoch 65/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7315 - subcellular_loss: 1.2942 - membrane_loss: 0.4373 - subcellular_accuracy: 0.5757 - membrane_accuracy: 0.7874 - val_loss: 2.0651 - val_subcellular_loss: 1.5449 - val_membrane_loss: 0.5202 - val_subcellular_accuracy: 0.5334 - val_membrane_accuracy: 0.7187\n",
      "Epoch 66/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7201 - subcellular_loss: 1.2878 - membrane_loss: 0.4322 - subcellular_accuracy: 0.5772 - membrane_accuracy: 0.7904 - val_loss: 2.0550 - val_subcellular_loss: 1.5408 - val_membrane_loss: 0.5141 - val_subcellular_accuracy: 0.5489 - val_membrane_accuracy: 0.7507\n",
      "Epoch 67/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7220 - subcellular_loss: 1.2843 - membrane_loss: 0.4377 - subcellular_accuracy: 0.5714 - membrane_accuracy: 0.7791 - val_loss: 2.1402 - val_subcellular_loss: 1.5934 - val_membrane_loss: 0.5468 - val_subcellular_accuracy: 0.5134 - val_membrane_accuracy: 0.6844\n",
      "Epoch 68/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7093 - subcellular_loss: 1.2759 - membrane_loss: 0.4333 - subcellular_accuracy: 0.5754 - membrane_accuracy: 0.7897 - val_loss: 2.0512 - val_subcellular_loss: 1.5363 - val_membrane_loss: 0.5149 - val_subcellular_accuracy: 0.5437 - val_membrane_accuracy: 0.7416\n",
      "Epoch 69/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.7186 - subcellular_loss: 1.2820 - membrane_loss: 0.4366 - subcellular_accuracy: 0.5779 - membrane_accuracy: 0.7832 - val_loss: 2.1378 - val_subcellular_loss: 1.5949 - val_membrane_loss: 0.5429 - val_subcellular_accuracy: 0.5249 - val_membrane_accuracy: 0.6924\n",
      "Epoch 70/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7232 - subcellular_loss: 1.2863 - membrane_loss: 0.4369 - subcellular_accuracy: 0.5756 - membrane_accuracy: 0.7887 - val_loss: 2.2583 - val_subcellular_loss: 1.6556 - val_membrane_loss: 0.6027 - val_subcellular_accuracy: 0.4397 - val_membrane_accuracy: 0.6021\n",
      "Epoch 71/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7309 - subcellular_loss: 1.2887 - membrane_loss: 0.4422 - subcellular_accuracy: 0.5708 - membrane_accuracy: 0.7826 - val_loss: 2.2680 - val_subcellular_loss: 1.6607 - val_membrane_loss: 0.6073 - val_subcellular_accuracy: 0.4626 - val_membrane_accuracy: 0.6032\n",
      "Epoch 72/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7050 - subcellular_loss: 1.2689 - membrane_loss: 0.4361 - subcellular_accuracy: 0.5796 - membrane_accuracy: 0.7848 - val_loss: 2.1585 - val_subcellular_loss: 1.5944 - val_membrane_loss: 0.5641 - val_subcellular_accuracy: 0.4929 - val_membrane_accuracy: 0.6507\n",
      "Epoch 73/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6877 - subcellular_loss: 1.2611 - membrane_loss: 0.4266 - subcellular_accuracy: 0.5832 - membrane_accuracy: 0.7933 - val_loss: 2.3127 - val_subcellular_loss: 1.6775 - val_membrane_loss: 0.6352 - val_subcellular_accuracy: 0.4071 - val_membrane_accuracy: 0.5946\n",
      "Epoch 74/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6988 - subcellular_loss: 1.2636 - membrane_loss: 0.4351 - subcellular_accuracy: 0.5821 - membrane_accuracy: 0.7869 - val_loss: 2.1749 - val_subcellular_loss: 1.5980 - val_membrane_loss: 0.5769 - val_subcellular_accuracy: 0.4585 - val_membrane_accuracy: 0.6478\n",
      "Epoch 75/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7049 - subcellular_loss: 1.2674 - membrane_loss: 0.4375 - subcellular_accuracy: 0.5772 - membrane_accuracy: 0.7836 - val_loss: 2.2376 - val_subcellular_loss: 1.6381 - val_membrane_loss: 0.5995 - val_subcellular_accuracy: 0.4534 - val_membrane_accuracy: 0.6072\n",
      "Epoch 76/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.7047 - subcellular_loss: 1.2709 - membrane_loss: 0.4338 - subcellular_accuracy: 0.5814 - membrane_accuracy: 0.7888 - val_loss: 2.2343 - val_subcellular_loss: 1.6350 - val_membrane_loss: 0.5993 - val_subcellular_accuracy: 0.4454 - val_membrane_accuracy: 0.6146\n",
      "Epoch 77/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.7021 - subcellular_loss: 1.2712 - membrane_loss: 0.4309 - subcellular_accuracy: 0.5799 - membrane_accuracy: 0.7894 - val_loss: 2.2511 - val_subcellular_loss: 1.6426 - val_membrane_loss: 0.6085 - val_subcellular_accuracy: 0.4345 - val_membrane_accuracy: 0.6118\n",
      "Epoch 78/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6882 - subcellular_loss: 1.2609 - membrane_loss: 0.4273 - subcellular_accuracy: 0.5837 - membrane_accuracy: 0.7947 - val_loss: 2.2867 - val_subcellular_loss: 1.6646 - val_membrane_loss: 0.6221 - val_subcellular_accuracy: 0.4197 - val_membrane_accuracy: 0.6192\n",
      "Epoch 79/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6901 - subcellular_loss: 1.2614 - membrane_loss: 0.4286 - subcellular_accuracy: 0.5854 - membrane_accuracy: 0.7924 - val_loss: 2.2323 - val_subcellular_loss: 1.6325 - val_membrane_loss: 0.5998 - val_subcellular_accuracy: 0.4385 - val_membrane_accuracy: 0.6152\n",
      "Epoch 80/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6985 - subcellular_loss: 1.2678 - membrane_loss: 0.4307 - subcellular_accuracy: 0.5785 - membrane_accuracy: 0.7843 - val_loss: 2.2535 - val_subcellular_loss: 1.6513 - val_membrane_loss: 0.6022 - val_subcellular_accuracy: 0.4334 - val_membrane_accuracy: 0.6118\n",
      "Epoch 81/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6831 - subcellular_loss: 1.2564 - membrane_loss: 0.4267 - subcellular_accuracy: 0.5847 - membrane_accuracy: 0.7965 - val_loss: 2.2280 - val_subcellular_loss: 1.6207 - val_membrane_loss: 0.6073 - val_subcellular_accuracy: 0.4471 - val_membrane_accuracy: 0.6083\n",
      "Epoch 82/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6815 - subcellular_loss: 1.2555 - membrane_loss: 0.4260 - subcellular_accuracy: 0.5886 - membrane_accuracy: 0.7934 - val_loss: 2.2598 - val_subcellular_loss: 1.6427 - val_membrane_loss: 0.6171 - val_subcellular_accuracy: 0.4362 - val_membrane_accuracy: 0.5969\n",
      "Epoch 83/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6800 - subcellular_loss: 1.2534 - membrane_loss: 0.4267 - subcellular_accuracy: 0.5857 - membrane_accuracy: 0.7950 - val_loss: 2.2483 - val_subcellular_loss: 1.6414 - val_membrane_loss: 0.6069 - val_subcellular_accuracy: 0.4391 - val_membrane_accuracy: 0.5981\n",
      "Epoch 84/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6798 - subcellular_loss: 1.2498 - membrane_loss: 0.4300 - subcellular_accuracy: 0.5801 - membrane_accuracy: 0.7916 - val_loss: 2.1566 - val_subcellular_loss: 1.5727 - val_membrane_loss: 0.5839 - val_subcellular_accuracy: 0.4717 - val_membrane_accuracy: 0.6392\n",
      "Epoch 85/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6688 - subcellular_loss: 1.2473 - membrane_loss: 0.4215 - subcellular_accuracy: 0.5866 - membrane_accuracy: 0.7972 - val_loss: 2.2022 - val_subcellular_loss: 1.6182 - val_membrane_loss: 0.5840 - val_subcellular_accuracy: 0.4808 - val_membrane_accuracy: 0.6238\n",
      "Epoch 86/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6864 - subcellular_loss: 1.2557 - membrane_loss: 0.4306 - subcellular_accuracy: 0.5893 - membrane_accuracy: 0.7939 - val_loss: 2.0858 - val_subcellular_loss: 1.5466 - val_membrane_loss: 0.5392 - val_subcellular_accuracy: 0.5152 - val_membrane_accuracy: 0.6878\n",
      "Epoch 87/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6624 - subcellular_loss: 1.2444 - membrane_loss: 0.4180 - subcellular_accuracy: 0.5886 - membrane_accuracy: 0.7927 - val_loss: 2.1575 - val_subcellular_loss: 1.5901 - val_membrane_loss: 0.5674 - val_subcellular_accuracy: 0.4860 - val_membrane_accuracy: 0.6444\n",
      "Epoch 88/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6696 - subcellular_loss: 1.2483 - membrane_loss: 0.4214 - subcellular_accuracy: 0.5854 - membrane_accuracy: 0.8004 - val_loss: 2.2861 - val_subcellular_loss: 1.6700 - val_membrane_loss: 0.6161 - val_subcellular_accuracy: 0.4351 - val_membrane_accuracy: 0.5855\n",
      "Epoch 89/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6652 - subcellular_loss: 1.2411 - membrane_loss: 0.4241 - subcellular_accuracy: 0.5918 - membrane_accuracy: 0.7937 - val_loss: 2.2976 - val_subcellular_loss: 1.6718 - val_membrane_loss: 0.6258 - val_subcellular_accuracy: 0.4271 - val_membrane_accuracy: 0.5763\n",
      "Epoch 90/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6800 - subcellular_loss: 1.2484 - membrane_loss: 0.4316 - subcellular_accuracy: 0.5841 - membrane_accuracy: 0.7878 - val_loss: 2.1979 - val_subcellular_loss: 1.6226 - val_membrane_loss: 0.5753 - val_subcellular_accuracy: 0.4723 - val_membrane_accuracy: 0.6387\n",
      "Epoch 91/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6637 - subcellular_loss: 1.2414 - membrane_loss: 0.4223 - subcellular_accuracy: 0.5832 - membrane_accuracy: 0.7976 - val_loss: 2.2649 - val_subcellular_loss: 1.6512 - val_membrane_loss: 0.6137 - val_subcellular_accuracy: 0.4254 - val_membrane_accuracy: 0.5912\n",
      "Epoch 92/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6726 - subcellular_loss: 1.2408 - membrane_loss: 0.4318 - subcellular_accuracy: 0.5898 - membrane_accuracy: 0.7861 - val_loss: 2.3617 - val_subcellular_loss: 1.7252 - val_membrane_loss: 0.6364 - val_subcellular_accuracy: 0.3865 - val_membrane_accuracy: 0.5700\n",
      "Epoch 93/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6438 - subcellular_loss: 1.2298 - membrane_loss: 0.4139 - subcellular_accuracy: 0.5938 - membrane_accuracy: 0.7999 - val_loss: 2.2306 - val_subcellular_loss: 1.6424 - val_membrane_loss: 0.5882 - val_subcellular_accuracy: 0.4545 - val_membrane_accuracy: 0.6026\n",
      "Epoch 94/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6504 - subcellular_loss: 1.2258 - membrane_loss: 0.4245 - subcellular_accuracy: 0.5890 - membrane_accuracy: 0.7931 - val_loss: 2.2106 - val_subcellular_loss: 1.6302 - val_membrane_loss: 0.5804 - val_subcellular_accuracy: 0.4757 - val_membrane_accuracy: 0.6198\n",
      "Epoch 95/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6573 - subcellular_loss: 1.2362 - membrane_loss: 0.4211 - subcellular_accuracy: 0.5869 - membrane_accuracy: 0.7969 - val_loss: 2.1072 - val_subcellular_loss: 1.5582 - val_membrane_loss: 0.5490 - val_subcellular_accuracy: 0.4906 - val_membrane_accuracy: 0.6707\n",
      "Epoch 96/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6542 - subcellular_loss: 1.2343 - membrane_loss: 0.4199 - subcellular_accuracy: 0.5850 - membrane_accuracy: 0.7917 - val_loss: 2.3481 - val_subcellular_loss: 1.6794 - val_membrane_loss: 0.6687 - val_subcellular_accuracy: 0.3934 - val_membrane_accuracy: 0.5580\n",
      "Epoch 97/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6577 - subcellular_loss: 1.2326 - membrane_loss: 0.4251 - subcellular_accuracy: 0.5941 - membrane_accuracy: 0.7931 - val_loss: 2.3230 - val_subcellular_loss: 1.6677 - val_membrane_loss: 0.6553 - val_subcellular_accuracy: 0.4168 - val_membrane_accuracy: 0.5597\n",
      "Epoch 98/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6654 - subcellular_loss: 1.2407 - membrane_loss: 0.4247 - subcellular_accuracy: 0.5856 - membrane_accuracy: 0.7917 - val_loss: 2.2219 - val_subcellular_loss: 1.6274 - val_membrane_loss: 0.5945 - val_subcellular_accuracy: 0.4505 - val_membrane_accuracy: 0.6038\n",
      "Epoch 99/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6520 - subcellular_loss: 1.2299 - membrane_loss: 0.4221 - subcellular_accuracy: 0.5905 - membrane_accuracy: 0.7905 - val_loss: 2.2309 - val_subcellular_loss: 1.6242 - val_membrane_loss: 0.6067 - val_subcellular_accuracy: 0.4488 - val_membrane_accuracy: 0.5941\n",
      "Epoch 100/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6436 - subcellular_loss: 1.2236 - membrane_loss: 0.4200 - subcellular_accuracy: 0.5948 - membrane_accuracy: 0.7986 - val_loss: 2.2631 - val_subcellular_loss: 1.6570 - val_membrane_loss: 0.6061 - val_subcellular_accuracy: 0.4488 - val_membrane_accuracy: 0.5872\n",
      "Epoch 101/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6525 - subcellular_loss: 1.2288 - membrane_loss: 0.4237 - subcellular_accuracy: 0.5913 - membrane_accuracy: 0.7916 - val_loss: 2.2726 - val_subcellular_loss: 1.6606 - val_membrane_loss: 0.6119 - val_subcellular_accuracy: 0.4511 - val_membrane_accuracy: 0.5849\n",
      "Epoch 102/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6165 - subcellular_loss: 1.2061 - membrane_loss: 0.4104 - subcellular_accuracy: 0.5937 - membrane_accuracy: 0.8005 - val_loss: 2.3037 - val_subcellular_loss: 1.6679 - val_membrane_loss: 0.6358 - val_subcellular_accuracy: 0.4282 - val_membrane_accuracy: 0.5723\n",
      "Epoch 103/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6382 - subcellular_loss: 1.2194 - membrane_loss: 0.4188 - subcellular_accuracy: 0.5941 - membrane_accuracy: 0.8001 - val_loss: 2.2394 - val_subcellular_loss: 1.6351 - val_membrane_loss: 0.6044 - val_subcellular_accuracy: 0.4631 - val_membrane_accuracy: 0.5975\n",
      "Epoch 104/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6360 - subcellular_loss: 1.2172 - membrane_loss: 0.4188 - subcellular_accuracy: 0.5937 - membrane_accuracy: 0.7923 - val_loss: 2.2361 - val_subcellular_loss: 1.6196 - val_membrane_loss: 0.6165 - val_subcellular_accuracy: 0.4362 - val_membrane_accuracy: 0.6089\n",
      "Epoch 105/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6308 - subcellular_loss: 1.2192 - membrane_loss: 0.4116 - subcellular_accuracy: 0.5967 - membrane_accuracy: 0.8024 - val_loss: 2.2913 - val_subcellular_loss: 1.6640 - val_membrane_loss: 0.6273 - val_subcellular_accuracy: 0.4248 - val_membrane_accuracy: 0.5832\n",
      "Epoch 106/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6322 - subcellular_loss: 1.2205 - membrane_loss: 0.4117 - subcellular_accuracy: 0.5955 - membrane_accuracy: 0.8047 - val_loss: 2.2013 - val_subcellular_loss: 1.6115 - val_membrane_loss: 0.5898 - val_subcellular_accuracy: 0.4677 - val_membrane_accuracy: 0.6192\n",
      "Epoch 107/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6391 - subcellular_loss: 1.2235 - membrane_loss: 0.4155 - subcellular_accuracy: 0.5885 - membrane_accuracy: 0.8004 - val_loss: 2.2388 - val_subcellular_loss: 1.6268 - val_membrane_loss: 0.6121 - val_subcellular_accuracy: 0.4443 - val_membrane_accuracy: 0.6009\n",
      "Epoch 108/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6140 - subcellular_loss: 1.2029 - membrane_loss: 0.4111 - subcellular_accuracy: 0.5995 - membrane_accuracy: 0.8018 - val_loss: 2.3842 - val_subcellular_loss: 1.6958 - val_membrane_loss: 0.6884 - val_subcellular_accuracy: 0.3974 - val_membrane_accuracy: 0.5540\n",
      "Epoch 109/120\n",
      "55/55 [==============================] - 4s 73ms/step - loss: 1.6371 - subcellular_loss: 1.2187 - membrane_loss: 0.4184 - subcellular_accuracy: 0.5924 - membrane_accuracy: 0.7926 - val_loss: 2.3859 - val_subcellular_loss: 1.7043 - val_membrane_loss: 0.6816 - val_subcellular_accuracy: 0.4094 - val_membrane_accuracy: 0.5678\n",
      "Epoch 110/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6276 - subcellular_loss: 1.2150 - membrane_loss: 0.4126 - subcellular_accuracy: 0.5905 - membrane_accuracy: 0.8028 - val_loss: 2.2691 - val_subcellular_loss: 1.6438 - val_membrane_loss: 0.6253 - val_subcellular_accuracy: 0.4545 - val_membrane_accuracy: 0.5843\n",
      "Epoch 111/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6190 - subcellular_loss: 1.2034 - membrane_loss: 0.4156 - subcellular_accuracy: 0.5968 - membrane_accuracy: 0.7978 - val_loss: 2.2159 - val_subcellular_loss: 1.6199 - val_membrane_loss: 0.5960 - val_subcellular_accuracy: 0.4877 - val_membrane_accuracy: 0.6032\n",
      "Epoch 112/120\n",
      "55/55 [==============================] - 4s 75ms/step - loss: 1.6451 - subcellular_loss: 1.2240 - membrane_loss: 0.4210 - subcellular_accuracy: 0.5905 - membrane_accuracy: 0.7910 - val_loss: 2.2130 - val_subcellular_loss: 1.6220 - val_membrane_loss: 0.5909 - val_subcellular_accuracy: 0.4780 - val_membrane_accuracy: 0.6141\n",
      "Epoch 113/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6303 - subcellular_loss: 1.2179 - membrane_loss: 0.4123 - subcellular_accuracy: 0.5979 - membrane_accuracy: 0.7991 - val_loss: 2.2847 - val_subcellular_loss: 1.6659 - val_membrane_loss: 0.6188 - val_subcellular_accuracy: 0.4688 - val_membrane_accuracy: 0.5889\n",
      "Epoch 114/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6219 - subcellular_loss: 1.2125 - membrane_loss: 0.4094 - subcellular_accuracy: 0.5966 - membrane_accuracy: 0.8018 - val_loss: 2.5618 - val_subcellular_loss: 1.7887 - val_membrane_loss: 0.7731 - val_subcellular_accuracy: 0.3728 - val_membrane_accuracy: 0.5300\n",
      "Epoch 115/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6208 - subcellular_loss: 1.2072 - membrane_loss: 0.4135 - subcellular_accuracy: 0.5957 - membrane_accuracy: 0.8023 - val_loss: 2.2288 - val_subcellular_loss: 1.6353 - val_membrane_loss: 0.5935 - val_subcellular_accuracy: 0.4648 - val_membrane_accuracy: 0.5992\n",
      "Epoch 116/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6065 - subcellular_loss: 1.1987 - membrane_loss: 0.4078 - subcellular_accuracy: 0.5987 - membrane_accuracy: 0.7981 - val_loss: 2.2160 - val_subcellular_loss: 1.6278 - val_membrane_loss: 0.5882 - val_subcellular_accuracy: 0.4700 - val_membrane_accuracy: 0.6106\n",
      "Epoch 117/120\n",
      "55/55 [==============================] - 4s 75ms/step - loss: 1.6158 - subcellular_loss: 1.2091 - membrane_loss: 0.4067 - subcellular_accuracy: 0.6000 - membrane_accuracy: 0.8062 - val_loss: 2.2010 - val_subcellular_loss: 1.6069 - val_membrane_loss: 0.5941 - val_subcellular_accuracy: 0.4666 - val_membrane_accuracy: 0.6032\n",
      "Epoch 118/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6131 - subcellular_loss: 1.2008 - membrane_loss: 0.4124 - subcellular_accuracy: 0.5993 - membrane_accuracy: 0.8034 - val_loss: 2.1403 - val_subcellular_loss: 1.5795 - val_membrane_loss: 0.5608 - val_subcellular_accuracy: 0.4974 - val_membrane_accuracy: 0.6535\n",
      "Epoch 119/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6081 - subcellular_loss: 1.1988 - membrane_loss: 0.4093 - subcellular_accuracy: 0.6032 - membrane_accuracy: 0.8036 - val_loss: 2.2887 - val_subcellular_loss: 1.6539 - val_membrane_loss: 0.6348 - val_subcellular_accuracy: 0.4277 - val_membrane_accuracy: 0.5780\n",
      "Epoch 120/120\n",
      "55/55 [==============================] - 4s 74ms/step - loss: 1.6104 - subcellular_loss: 1.2011 - membrane_loss: 0.4092 - subcellular_accuracy: 0.5977 - membrane_accuracy: 0.7992 - val_loss: 2.2001 - val_subcellular_loss: 1.6088 - val_membrane_loss: 0.5914 - val_subcellular_accuracy: 0.4900 - val_membrane_accuracy: 0.6078\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s18N5J5LmaMM",
    "outputId": "fde53335-61ae-4c36-b440-64265c169af7"
   },
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "y_pred = np.argmax(Y_pred[1], axis=1)\n",
    "MCC = matthews_corrcoef(validation['y_test_membrane'][1:], y_pred)\n",
    "Y_pred = model.predict(X_val)\n",
    "y_pred = np.argmax(Y_pred[1], axis=1)\n",
    "gorodkin = matthews_corrcoef(validation['y_test_location'][1:], y_pred)\n",
    "\n",
    "print(\"Minimum subcellular validation loss: {:.6f}\".format(min(history.history['val_subcellular_loss'])))\n",
    "acc_index = np.argmin(history.history['val_subcellular_loss'])\n",
    "print(\"With subcellular accuracy: {:.6f}\".format(history.history['val_subcellular_accuracy'][acc_index]))\n",
    "print(\"Minimum membrane validation loss: {:.6f}\".format(history.history['val_membrane_loss'][acc_index]))\n",
    "print(\"With membrane accuracy: {:.6f}\".format(history.history['val_membrane_accuracy'][acc_index]))\n",
    "print(\"MCC: \" + str(MCC))\n",
    "print(\"Gorodkin: \" + str(gorodkin))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Minimum subcellular validation loss: 1.494079\n",
      "With subcellular accuracy: 0.549457\n",
      "Minimum membrane validation loss: 0.491265\n",
      "With membrane accuracy: 0.776444\n",
      "MCC: 0.33383296895130793\n",
      "Gorodkin: 0.11090313935883533\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}